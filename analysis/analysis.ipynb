{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import gym\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pirl\n",
    "from pirl import config\n",
    "from pirl.experiments import experiments\n",
    "\n",
    "from analysis import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "experiment = 'dummy-test-20180520_185729-e7ea21a03a6d0c94184d024631196687a4c018a7'\n",
    "experiment_dir = osp.join('..', 'data', 'experiments', experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_pattern = '(.*)'\n",
    "env_pattern = '(.*)'\n",
    "df = common.load_value(experiment_dir, algo_pattern, env_pattern)\n",
    "mean, se = common.aggregate_value(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common.plot_ci(mean, se)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expert_cached_value(rl, env_name, pol_discount=0.99, eval_discount=1.00, seed=1234, episodes=100):\n",
    "    '''Rollout a cached expert policy for episodes.\n",
    "       WARNING: This will be slow or just break if policy is not in cache!'''\n",
    "    gen_policy, _sample, compute_value = config.RL_ALGORITHMS[rl]\n",
    "    policy, value = experiments._train_policy(rl, pol_discount, env_name, seed, None)\n",
    "    vmean, vse = value\n",
    "    print('Cached value: {:.3f} +/- {:.3f}'.format(vmean, 1.96 * vse))\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    rmean, rse = compute_value(env, policy, eval_discount, num_episodes=episodes, seed=seed)\n",
    "    print('Rollout value: {:.3f} +/- {:.3f}'.format(rmean, 1.96 * rse))\n",
    "    return (vmean, vse), (rmean, rse)\n",
    "\n",
    "def _policy_value(results_dir, rl, env_name, pol_discount, eval_discount, episodes, seed):\n",
    "    _gen_policy, _sample, compute_value = config.RL_ALGORITHMS[rl]\n",
    "    fname = osp.join(results_dir, 'policy.pkl')\n",
    "    print('Loading policy from ', fname)\n",
    "    policy = joblib.load(fname)\n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    mean, se = compute_value(env, policy, eval_discount, num_episodes=episodes, seed=seed)\n",
    "    print('Rollout value: {:.3f} +/- {:.3f}'.format(mean, 1.96 * se))\n",
    "    return mean, se\n",
    "\n",
    "def expert_value(experiment_dir, rl, env_name, pol_discount=0.99, eval_discount=1.00, episodes=100, seed=1234):\n",
    "    results_dir = osp.join(experiment_dir, 'expert', env_name, rl)\n",
    "    return _policy_value(results_dir, rl, env_name, pol_discount, eval_discount, episodes, seed)\n",
    "\n",
    "def irl_eval_value(experiment_dir, irl_name, num_traj, rl, env_name, pol_discount=0.99, eval_discount=1.00, episodes=100, seed=1234):\n",
    "    results_dir = osp.join(experiment_dir, 'eval', env_name, \n",
    "                           '{}:{}:{}'.format(irl_name, num_traj, num_traj), rl)\n",
    "    return _policy_value(results_dir, rl, env_name, pol_discount, eval_discount, episodes, seed)\n",
    "    \n",
    "def irl_value(experiment_dir, irl_name, env_name, num_traj, eval_discount=1.00, episodes=100):\n",
    "    _irl_algo, _reward_wrapper, compute_value = experiments.make_irl_algo(irl_name)\n",
    "    irl_dir = osp.join(experiment_dir, 'irl', irl_name)\n",
    "    if not os.path.exists(irl_dir):\n",
    "        raise FileNotFoundError(\"No result directory {}\".format(irl_dir))\n",
    "    \n",
    "    pop_fname = osp.join(irl_dir, str(num_traj), 'policies.pkl')\n",
    "    sin_fname = osp.join(irl_dir, env_name, str(num_traj), 'policy.pkl')\n",
    "    if os.path.exists(pop_fname):\n",
    "        policies = joblib.load(pop_fname)\n",
    "        print(policies.keys())\n",
    "        policy = policies[env_name]\n",
    "    elif os.path.exists(sin_fname):\n",
    "        policy = joblib.load(sin_fname)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Neither {} or {} exists\".format(pop_fname, sin_fname))\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    mean, se = compute_value(env, policy, discount=eval_discount, num_episodes=episodes)\n",
    "    print('Rollout value: {} +/- {}'.format(mean, 1.96 * se))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_cached_value('ppo_cts', 'Reacher-v2', episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_value(experiment_dir, 'ppo_cts', 'Reacher-v2', episodes=500, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irl_value(experiment_dir, 'airl', 'Reacher-v2', 1000, episodes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing rewards (gridworld only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_heatmaps(irl_algo, kind='inline', out_dir=None, shape=(9,9), **kwargs):\n",
    "    data = pd.read_pickle(osp.join(experiment_dir, 'results.pkl'))\n",
    "    rewards = data['rewards'][irl_algo]\n",
    "    if kind in ['inline', 'pdf']:\n",
    "        figs = common.gridworld_heatmap(rewards, shape)\n",
    "        if out_dir is None:\n",
    "            for fig in figs:\n",
    "                display(fig[1])\n",
    "        else:\n",
    "            common.save_figs(figs, out_dir)\n",
    "    elif kind == 'movie':\n",
    "        common.gridworld_heatmap_movie(out_dir, rewards, shape)\n",
    "    else:\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irl_algos = ['mce', 'mcep_reg1e0']\n",
    "for irl in irl_algos:\n",
    "    show_heatmaps(irl, kind='inline', shape=(4,4))\n",
    "    #show_heatmaps(irl, kind='movie', out_dir='figs/jungle/movies/' + irl)\n",
    "    #show_heatmaps(irl, kind='movie', out_dir='figs/jungle/' + irl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss curve (PPO only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_progress(results_dir):\n",
    "    path = osp.join(results_dir, 'progress.csv')\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.set_index('serial_timesteps')\n",
    "    return df\n",
    "\n",
    "def expert_ppo_progress(experiment_dir, env_name, rl_name):\n",
    "    results_dir = osp.join(experiment_dir, 'expert', \n",
    "                       experiments.sanitize_env_name(env_name),\n",
    "                       rl_name)\n",
    "    return ppo_progress(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = ['InvertedPendulum-v2', 'InvertedDoublePendulum-v2', 'Reacher-v2']\n",
    "for env in envs:\n",
    "    df = expert_ppo_progress(experiment_dir, env, 'ppo_cts')\n",
    "    plt.figure()\n",
    "    df['eprewmean'].plot()\n",
    "    print(df['eprewmean'].max())\n",
    "    plt.title(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(results_dir, checkpoint_num=None):\n",
    "    checkpoint_dir = osp.join(results_dir, 'checkpoints')\n",
    "    if checkpoint_num is None:\n",
    "        checkpoint_num = max(os.listdir(checkpoint_dir))\n",
    "    checkpoint_fname = osp.join(checkpoint_dir, checkpoint_num)\n",
    "    print('Loading from ', checkpoint_fname)\n",
    "    policy = joblib.load(checkpoint_fname)\n",
    "    \n",
    "    return policy\n",
    "\n",
    "def ppo_value2(results_dir, rl, env_name, pol_discount, eval_discount, episodes):\n",
    "    _gen_policy, _sample, compute_value = config.RL_ALGORITHMS[rl]\n",
    "    fname = osp.join(results_dir, 'policy.pkl')\n",
    "    print('Loading policy from ', fname)\n",
    "    policy = joblib.load(fname)\n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    mean, se = compute_value(env, policy, eval_discount, num_episodes=episodes)\n",
    "    print('Rollout value: {:.3f} +/- {:.3f}'.format(mean, 1.96 * se))\n",
    "    return mean, se\n",
    "\n",
    "def ppo_value(policy, env_name, episodes, seed=1234):\n",
    "    _, _, compute_value = config.RL_ALGORITHMS['ppo_cts']\n",
    "    env = gym.make(env_name)\n",
    "    mean, se = compute_value(env, policy, 1.00, num_episodes=episodes, seed=seed)\n",
    "    print('Rollout value: {:.3f} +/- {:.3f}'.format(mean, 1.96 * se))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = load_checkpoint(osp.join(experiment_dir, 'expert/InvertedPendulum-v2/ppo_cts'), '00488')\n",
    "ppo_value(policy, 'InvertedPendulum-v2', 500, 1234)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
