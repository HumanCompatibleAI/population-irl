{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import collections\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import gym\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pirl\n",
    "from pirl import config\n",
    "from pirl.experiments import experiments\n",
    "\n",
    "from analysis import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "experiments = [\n",
    "    'few-jungle-9x9-Liquid-20180521_041300-ddc8132bb2a30ef149e3b567d84ccacd168e045c',\n",
    "    'few-jungle-9x9-Water-20180521_041259-ddc8132bb2a30ef149e3b567d84ccacd168e045c',\n",
    "    'few-jungle-9x9-Soda-20180521_041300-ddc8132bb2a30ef149e3b567d84ccacd168e045c',\n",
    "]\n",
    "experiments_dir = [osp.join('..', 'data', 'aws', 'experiments', x) for x in experiments]\n",
    "figs_dir = osp.join('../../population-irl-paper', 'figs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_pattern = '(.*)'\n",
    "env_pattern = '.*-(.*)-v0'\n",
    "dfs = [common.load_value(x, algo_pattern, env_pattern) for x in experiments_dir]\n",
    "df = pd.concat(dfs)\n",
    "values = common.aggregate_value(df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common.plot_ci(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expert_cached_value(rl, env_name, pol_discount=0.99, eval_discount=1.00, seed=1234, episodes=100):\n",
    "    '''Rollout a cached expert policy for episodes.\n",
    "       WARNING: This will be slow or just break if policy is not in cache!'''\n",
    "    gen_policy, _sample, compute_value = config.RL_ALGORITHMS[rl]\n",
    "    policy, value = experiments._train_policy(rl, pol_discount, env_name, seed, None)\n",
    "    vmean, vse = value\n",
    "    print('Cached value: {:.3f} +/- {:.3f}'.format(vmean, 1.96 * vse))\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    rmean, rse = compute_value(env, policy, eval_discount, num_episodes=episodes, seed=seed)\n",
    "    print('Rollout value: {:.3f} +/- {:.3f}'.format(rmean, 1.96 * rse))\n",
    "    return (vmean, vse), (rmean, rse)\n",
    "\n",
    "def _policy_value(results_dir, rl, env_name, pol_discount, eval_discount, episodes, seed):\n",
    "    _gen_policy, _sample, compute_value = config.RL_ALGORITHMS[rl]\n",
    "    fname = osp.join(results_dir, 'policy.pkl')\n",
    "    print('Loading policy from ', fname)\n",
    "    policy = joblib.load(fname)\n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    mean, se = compute_value(env, policy, eval_discount, num_episodes=episodes, seed=seed)\n",
    "    print('Rollout value: {:.3f} +/- {:.3f}'.format(mean, 1.96 * se))\n",
    "    return mean, se\n",
    "\n",
    "def expert_value(experiment_dir, rl, env_name, pol_discount=0.99, eval_discount=1.00, episodes=100, seed=1234):\n",
    "    results_dir = osp.join(experiment_dir, 'expert', env_name, rl)\n",
    "    return _policy_value(results_dir, rl, env_name, pol_discount, eval_discount, episodes, seed)\n",
    "\n",
    "def irl_eval_value(experiment_dir, irl_name, num_traj, rl, env_name, pol_discount=0.99, eval_discount=1.00, episodes=100, seed=1234):\n",
    "    results_dir = osp.join(experiment_dir, 'eval', env_name, \n",
    "                           '{}:{}:{}'.format(irl_name, num_traj, num_traj), rl)\n",
    "    return _policy_value(results_dir, rl, env_name, pol_discount, eval_discount, episodes, seed)\n",
    "    \n",
    "def irl_value(experiment_dir, irl_name, env_name, num_traj, eval_discount=1.00, episodes=100):\n",
    "    _irl_algo, _reward_wrapper, compute_value = experiments.make_irl_algo(irl_name)\n",
    "    irl_dir = osp.join(experiment_dir, 'irl', irl_name)\n",
    "    if not os.path.exists(irl_dir):\n",
    "        raise FileNotFoundError(\"No result directory {}\".format(irl_dir))\n",
    "    \n",
    "    pop_fname = osp.join(irl_dir, str(num_traj), 'policies.pkl')\n",
    "    sin_fname = osp.join(irl_dir, env_name, str(num_traj), 'policy.pkl')\n",
    "    if os.path.exists(pop_fname):\n",
    "        policies = joblib.load(pop_fname)\n",
    "        print(policies.keys())\n",
    "        policy = policies[env_name]\n",
    "    elif os.path.exists(sin_fname):\n",
    "        policy = joblib.load(sin_fname)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Neither {} or {} exists\".format(pop_fname, sin_fname))\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    mean, se = compute_value(env, policy, discount=eval_discount, num_episodes=episodes)\n",
    "    print('Rollout value: {} +/- {}'.format(mean, 1.96 * se))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_cached_value('ppo_cts', 'Reacher-v2', episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_value(experiment_dir, 'ppo_cts', 'Reacher-v2', episodes=500, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irl_value(experiment_dir, 'airl', 'Reacher-v2', 1000, episodes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing rewards (gridworld only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_heatmaps(irl_algo, kind='inline', out_dir=None, shape=(9,9), **kwargs):\n",
    "    data = pd.read_pickle(osp.join(experiment_dir, 'results.pkl'))\n",
    "    rewards = data['rewards'][irl_algo]\n",
    "    if kind in ['inline', 'pdf']:\n",
    "        figs = myplots.gridworld_heatmap(rewards, shape)\n",
    "        if out_dir is None:\n",
    "            for fig in figs:\n",
    "                display(fig[1])\n",
    "        else:\n",
    "            myplots.save_figs(figs, out_dir)\n",
    "    elif kind == 'movie':\n",
    "        myplots.gridworld_heatmap_movie(out_dir, rewards, shape)\n",
    "    else:\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irl_algos = ['mce', 'mcec', 'mcep_reg1e0', 'mcep_reg1e-1', 'mcep_reg1e-2']\n",
    "for irl in irl_algos:\n",
    "    show_heatmaps(irl, kind='pdf', out_dir='figs/few-jungle/' + irl, shape=(9,9))\n",
    "    #show_heatmaps(irl, kind='movie', out_dir='figs/jungle/movies/' + irl)\n",
    "    #show_heatmaps(irl, kind='movie', out_dir='figs/jungle/' + irl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jungle experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jungle_types = collections.OrderedDict([\n",
    "    ('A', 'Soda'), \n",
    "    ('B', 'Water'), \n",
    "    ('A+B', 'Liquid')\n",
    "])\n",
    "jungle_envs = collections.OrderedDict([\n",
    "    (k, 'pirl/GridWorld-Jungle-9x9-{}-v0'.format(v))\n",
    "    for k, v in jungle_types.items()\n",
    "])\n",
    "default_algos = collections.OrderedDict([\n",
    "    ('mce', 'Single'),\n",
    "    ('mcec', 'Joint Training'),\n",
    "    ('mcep reg1e-1', 'Multi-Task'),\n",
    "    ('value iteration', 'Oracle'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context([common.style('default'), common.style('twocol')]):\n",
    "    fig = common.gridworld_ground_truth(jungle_envs, (9,9))\n",
    "    fig.savefig(osp.join(figs_dir, 'jungle', 'gt.pdf'))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartoon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context([common.style('default'), common.style('onecol')]):\n",
    "    fig = common.gridworld_cartoon((9,9))\n",
    "    fig.savefig(osp.join(figs_dir, 'jungle', 'cartoon.pdf'))\n",
    "    #plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jungle_val = df.stack().unstack('seed').max(axis=1).unstack(-1)\n",
    "jungle_val = jungle_val.rename(index={v: k for k, v in jungle_types.items()}, level=0)\n",
    "jungle_val = jungle_val.xs((1000, 'value_iteration'), level=('n', 'eval'))\n",
    "baseline_comparison = jungle_val.rename(columns=default_algos).loc[:, default_algos.values()]\n",
    "with plt.style.context([common.style('default'), common.style('twocol')]):\n",
    "    fig = common.value_bar_chart_by_env(baseline_comparison, envs=jungle_types.keys(), relative='Oracle')\n",
    "    fig.savefig(osp.join(figs_dir, 'jungle', 'baseline_comparison.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_algorithms = collections.OrderedDict([\n",
    "    #('mcep reg1e-4', '$\\lambda = 10^{-4}$'),\n",
    "    #('mcep reg1e-3', '$\\lambda = 10^{-3}$'),\n",
    "    ('mcep reg1e-2', '$\\lambda = 10^{-2}$'),\n",
    "    ('mcep reg1e-1', '$\\lambda = 10^{-1}$'),\n",
    "    ('mcep reg1e0', '$\\lambda = 10^0$'),\n",
    "    ('value iteration', 'Oracle'),\n",
    "])\n",
    "jungle_val = values.rename(index={v: k for k, v in jungle_types.items()}, level=0)\n",
    "jungle_val = jungle_val.xs((1000, 'value_iteration'), level=('n', 'eval'))\n",
    "reg_comparison = jungle_val.rename(columns=reg_algorithms).loc[:, reg_algorithms.values()]\n",
    "with plt.style.context([common.style('default'), common.style('twocol')]):\n",
    "    fig = common.value_bar_chart_by_env(reg_comparison, envs=jungle_types.keys(), \n",
    "                                         relative='Oracle', cmap=cm.get_cmap('Accent'))\n",
    "    fig.savefig(osp.join(figs_dir, 'jungle', 'reg_comparison.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = common.plot_ci(values, dp=1)\n",
    "#ci = ci.str.replace('nan +/- nan', '---')\n",
    "ci = ci.rename(index={v: k for k, v in jungle_types.items()}, level=0)\n",
    "ci = ci.rename(columns=default_algos).loc[:, default_algos.values()]\n",
    "ci.columns.name = 'algo'\n",
    "#ci = ci.unstack('env')\n",
    "#ci.columns = ci.columns.reorder_levels(['env', 'algo'])\n",
    "#envs = ci.columns.levels[0]\n",
    "#ci = pd.concat([ci.loc[:, [x]] for x in envs], axis=1)\n",
    "ci = ci.xs((1000, 'value_iteration'), level=('n', 'eval'))\n",
    "print(ci.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_values = values.rename(columns=default_algos).loc[:, default_algos.values()]\n",
    "print(common.value_latex_table(table_values, envs=['A', 'B', 'A+B'])), relative='Oracle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ad-hoc experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = osp.join(experiment_dir, 'results.pkl')\n",
    "data = pd.read_pickle(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'pirl/GridWorld-Jungle-9x9-Soda-v0'\n",
    "algo = 'mce'\n",
    "fig, axs = plt.subplots(1, 2, figsize=(13, 6))\n",
    "# zeroshot = data['rewards'][algo][1000][0][env_name]\n",
    "# oneshot = data['rewards'][algo][1000][1][env_name]\n",
    "# myplots._gridworld_heatmap(oneshot - zeroshot, (9,9), fmt='.2f', ax=axs[0])\n",
    "fiveshot = data['rewards'][algo][1000][5][env_name]\n",
    "hundredshot = data['rewards'][algo][1000][100][env_name]\n",
    "myplots._gridworld_heatmap(hundredshot - zeroshot, (9,9), fmt='.2f', ax=axs[1])\n",
    "\n",
    "env = gym.make(env_name)\n",
    "gt = env.unwrapped.reward\n",
    "env.close()\n",
    "r = data['rewards'][algo][1000]\n",
    "reward_delta = {k: np.linalg.norm(v[env_name] - np.mean(v[env_name]) - gt + np.mean(gt))\n",
    "                for k, v in r.items()}\n",
    "reward_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
