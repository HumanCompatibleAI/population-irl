{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Logging to /tmp/ppo-direct/ppo\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0057388837 |\n",
      "| clipfrac           | 0.06821289   |\n",
      "| eplenmean          | 50           |\n",
      "| eprewmean          | -111         |\n",
      "| explained_variance | -0.299       |\n",
      "| fps                | 419          |\n",
      "| nupdates           | 1            |\n",
      "| policy_entropy     | 2.8149617    |\n",
      "| policy_loss        | -0.010011508 |\n",
      "| serial_timesteps   | 2048         |\n",
      "| time_elapsed       | 4.88         |\n",
      "| total_timesteps    | 2048         |\n",
      "| value_loss         | 0.39306355   |\n",
      "-------------------------------------\n",
      "Saving to /tmp/ppo-direct/ppo/checkpoints/00001\n",
      "Updating model due to mean reward increase: None -> -110.782727125\n",
      "-------------------------------------\n",
      "| approxkl           | 0.006204863  |\n",
      "| clipfrac           | 0.069726564  |\n",
      "| eplenmean          | 50           |\n",
      "| eprewmean          | -110         |\n",
      "| explained_variance | -0.703       |\n",
      "| fps                | 468          |\n",
      "| nupdates           | 2            |\n",
      "| policy_entropy     | 2.7617247    |\n",
      "| policy_loss        | -0.010394953 |\n",
      "| serial_timesteps   | 4096         |\n",
      "| time_elapsed       | 9.28         |\n",
      "| total_timesteps    | 4096         |\n",
      "| value_loss         | 0.27564794   |\n",
      "-------------------------------------\n",
      "Updating model due to mean reward increase: -110.782727125 -> -109.62059653086419\n",
      "-------------------------------------\n",
      "| approxkl           | 0.006537015  |\n",
      "| clipfrac           | 0.06723633   |\n",
      "| eplenmean          | 50           |\n",
      "| eprewmean          | -108         |\n",
      "| explained_variance | -0.408       |\n",
      "| fps                | 428          |\n",
      "| nupdates           | 3            |\n",
      "| policy_entropy     | 2.6896558    |\n",
      "| policy_loss        | -0.010520937 |\n",
      "| serial_timesteps   | 6144         |\n",
      "| time_elapsed       | 14.1         |\n",
      "| total_timesteps    | 6144         |\n",
      "| value_loss         | 0.33033425   |\n",
      "-------------------------------------\n",
      "Updating model due to mean reward increase: -109.62059653086419 -> -108.30690171\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0047634663 |\n",
      "| clipfrac           | 0.043017577  |\n",
      "| eplenmean          | 50           |\n",
      "| eprewmean          | -102         |\n",
      "| explained_variance | -0.404       |\n",
      "| fps                | 427          |\n",
      "| nupdates           | 4            |\n",
      "| policy_entropy     | 2.6166909    |\n",
      "| policy_loss        | -0.0084953   |\n",
      "| serial_timesteps   | 8192         |\n",
      "| time_elapsed       | 18.9         |\n",
      "| total_timesteps    | 8192         |\n",
      "| value_loss         | 0.256635     |\n",
      "-------------------------------------\n",
      "Saving to /tmp/ppo-direct/ppo/checkpoints/00004\n",
      "Updating model due to mean reward increase: -108.30690171 -> -101.99565588\n",
      "PPO direct ran in  19.323145389556885\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "2018-04-15 16:43:35.833767 PDT | observation space: Box(11,)\n",
      "2018-04-15 16:43:35.834469 PDT | action space: Box(2,)\n",
      "Logging to /tmp/ppo-direct/ppo\n",
      "\u001b[33mWARN: <class 'pirl.irl.airl.AIRLRewardWrapper'> doesn't implement 'reset' method, which is required for wrappers derived directly from Wrapper. Deprecated default implementation is used.\u001b[0m\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0044819256  |\n",
      "| clipfrac           | 0.050634764   |\n",
      "| eplenmean          | 50            |\n",
      "| eprewmean          | -106          |\n",
      "| explained_variance | -0.384        |\n",
      "| fps                | 333           |\n",
      "| nupdates           | 1             |\n",
      "| policy_entropy     | 2.8370214     |\n",
      "| policy_loss        | -0.0066555166 |\n",
      "| serial_timesteps   | 2048          |\n",
      "| time_elapsed       | 6.15          |\n",
      "| total_timesteps    | 2048          |\n",
      "| value_loss         | 0.41266274    |\n",
      "--------------------------------------\n",
      "Saving to /tmp/ppo-direct/ppo/checkpoints/00001\n",
      "Updating model due to mean reward increase: None -> -106.29758084999999\n",
      "-------------------------------------\n",
      "| approxkl           | 0.006182133  |\n",
      "| clipfrac           | 0.08242188   |\n",
      "| eplenmean          | 50           |\n",
      "| eprewmean          | -104         |\n",
      "| explained_variance | -0.87        |\n",
      "| fps                | 346          |\n",
      "| nupdates           | 2            |\n",
      "| policy_entropy     | 2.840506     |\n",
      "| policy_loss        | -0.012910755 |\n",
      "| serial_timesteps   | 4096         |\n",
      "| time_elapsed       | 12.1         |\n",
      "| total_timesteps    | 4096         |\n",
      "| value_loss         | 0.31302732   |\n",
      "-------------------------------------\n",
      "Updating model due to mean reward increase: -106.29758084999999 -> -104.1880192469136\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0067659533 |\n",
      "| clipfrac           | 0.08935547   |\n",
      "| eplenmean          | 50           |\n",
      "| eprewmean          | -97.9        |\n",
      "| explained_variance | -0.0728      |\n",
      "| fps                | 347          |\n",
      "| nupdates           | 3            |\n",
      "| policy_entropy     | 2.8249085    |\n",
      "| policy_loss        | -0.010347289 |\n",
      "| serial_timesteps   | 6144         |\n",
      "| time_elapsed       | 18           |\n",
      "| total_timesteps    | 6144         |\n",
      "| value_loss         | 0.16034926   |\n",
      "-------------------------------------\n",
      "Updating model due to mean reward increase: -104.1880192469136 -> -97.87089081\n",
      "-------------------------------------\n",
      "| approxkl           | 0.006818244  |\n",
      "| clipfrac           | 0.08642578   |\n",
      "| eplenmean          | 50           |\n",
      "| eprewmean          | -91.5        |\n",
      "| explained_variance | 0.0773       |\n",
      "| fps                | 353          |\n",
      "| nupdates           | 4            |\n",
      "| policy_entropy     | 2.7895186    |\n",
      "| policy_loss        | -0.010102796 |\n",
      "| serial_timesteps   | 8192         |\n",
      "| time_elapsed       | 23.8         |\n",
      "| total_timesteps    | 8192         |\n",
      "| value_loss         | 0.12045531   |\n",
      "-------------------------------------\n",
      "Saving to /tmp/ppo-direct/ppo/checkpoints/00004\n",
      "Updating model due to mean reward increase: -97.87089081 -> -91.5144568\n",
      "PPO on AIRL ran in  24.620743989944458\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "import time\n",
    "\n",
    "import joblib\n",
    "import gym\n",
    "\n",
    "from pirl.agents import ppo\n",
    "from pirl.irl import airl\n",
    "from pirl.experiments import config\n",
    "\n",
    "env_name = 'Reacher-v2'\n",
    "fname = 'data/dummy-continuous-test-20180415_161213-df2749c7662b658361ca1881f3476f78fd4a5d31/irl/airl_quicks/20/'\n",
    "\n",
    "def test(reward=None):\n",
    "    env = gym.make(env_name)\n",
    "    env.seed(0)\n",
    "    start = time.time()\n",
    "    if reward is not None:\n",
    "        env = airl.AIRLRewardWrapper(env, reward, tf_cfg=config.TENSORFLOW)\n",
    "    ppo.train_continuous(env, 0.99, '/tmp/ppo-direct', \n",
    "                         tf_config=config.TENSORFLOW, \n",
    "                         num_timesteps=1e4)\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    env.close()\n",
    "    return elapsed\n",
    "\n",
    "print('PPO direct ran in ', test())\n",
    "reward = joblib.load(osp.join(fname, 'rewards.pkl'))[env_name]\n",
    "print('PPO on AIRL ran in ', test(reward))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
