cluster_name: pirl
# Autoscaler
min_workers: 0
max_workers: 40
target_utilization_fraction: 0.9
idle_timeout_minutes: 5

# General AWS Config
provider:
    type: aws
    region: us-west-2
    availability_zone: us-west-2a,us-west-2b,us-west-2c  # this AZ is generally the cheapest
auth:
    ssh_user: ec2-user
    ssh_private_key: ./config/population-irl.pem

# Node templates
head_node:
    #TODO: when Ray issue #2106 is fixed, switch this to non-GPU (e.g. m5)
    InstanceType: p2.xlarge
    ImageId: ami-07a370ec376c186a2  # Custom AMI with our code pre-installed
    KeyName: Population-IRL

file_mounts: {
    "/tmp/current_branch_sha": "./.git/refs/heads/master",
}

worker_nodes:
    # IMPORTANT: If you update the instance type, also update --num-gpus
    # in worker_start_ray_commands
    InstanceType: p2.xlarge
    ImageId: ami-07a370ec376c186a2  # Custom AMI with our code pre-installed
    KeyName: Population-IRL
    InstanceMarketOptions:
        MarketType: spot  # max price defaults to the on-demand price

# List of shell commands to run to set up nodes.
setup_commands:
    # Note we don't update dependencies. This improves start-up speed,
    # but means we need to create a new AMI each time dependencies change.
    - cd $HOME/population-irl && git fetch && git checkout `cat /tmp/current_branch_sha`
    - ln -sf $HOME/population-irl/pirl/config/cloud.py $HOME/population-irl/pirl/config/config_local.py
head_setup_commands: []
worker_setup_commands: []

head_start_ray_commands:
    # Set up alarm for if node is left running by mistake
    - $HOME/population-irl/scripts/cloudwatch_alarms.sh
    - ray stop
    # (Re)start our Redis cache instance
    - redis-cli -p 6380 shutdown || true
    - redis-server $HOME/population-irl/config/redis/cluster.conf
    # Pretend we have more GPUs to workaround Ray issue #402.
    - ulimit -n 65536; AUTOSCALER_MAX_LAUNCH_BATCH=1 BOTO_MAX_RETRIES=5 ray start --num-gpus=2 --num-cpus=2 --head --redis-port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml

# Command to start ray on worker nodes. You don't need to change this.
worker_start_ray_commands:
    - ray stop
    # (Re)start spot termination monitor
    - pkill -x -f "python $HOME/population-irl/scripts/aws_termination.py" || true
    - python $HOME/population-irl/scripts/aws_termination.py >> /tmp/aws_termination.log 2>&1 & disown
    # Pretend we have more GPUs to workaround Ray issue #402.
    # IMPORTANT: Increase this amount if you have a better GPU!
    # Only 4 CPUs, but allow for some oversubscription. 
    - ulimit -n 65536; ray start --num-cpus=6 --num-gpus=6 --redis-address=$RAY_HEAD_IP:6379 --object-manager-port=8076
