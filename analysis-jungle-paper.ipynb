{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import collections\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import gym\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pirl\n",
    "from pirl.experiments import config, experiments, plots as myplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "experiment = 'few-jungle-20180508_170607-0f47c1243e034ef403d65d8db95c256c3ed75f3b'\n",
    "experiment_dir = osp.join('data', experiment)\n",
    "figs_dir = osp.join('../population-irl-paper', 'figs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value(experiment_dir, algo_pattern='(.*)', env_pattern='(.*)', algos=['.*'], dps=2):\n",
    "    fname = osp.join(experiment_dir, 'results.pkl')\n",
    "    data = pd.read_pickle(fname)\n",
    "    \n",
    "    value = myplots.extract_value(data)\n",
    "    value.columns = value.columns.str.extract(algo_pattern, expand=False)\n",
    "    envs = value.index.levels[0].str.extract(env_pattern, expand=False)\n",
    "    value.index = value.index.set_levels(envs, level=0)\n",
    "    \n",
    "    matches = []\n",
    "    mask = pd.Series(False, index=value.columns)\n",
    "    for p in algos:\n",
    "        m = value.columns.str.match(p)\n",
    "        matches += list(value.columns[m & (~mask)])\n",
    "        mask |= m\n",
    "    value = value.loc[:, matches]\n",
    "    \n",
    "    value.columns = value.columns.str.split('_').str.join(' ')  # so lines wrap\n",
    "    value = value.round(dps)\n",
    "    return value\n",
    "\n",
    "def plot_ci(df):\n",
    "    mean = df.loc[(slice(None), slice(None), slice(None), slice(None), 'mean'), :]\n",
    "    se = df.loc[(slice(None), slice(None), slice(None), slice(None), 'se'), :]\n",
    "    mean.index = mean.index.droplevel('type')\n",
    "    se.index = se.index.droplevel('type')\n",
    "    return mean.applymap(lambda x: '{:.3f} +/- '.format(x)) + se.applymap(lambda x: '{:.3f}'.format(1.96 * x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_pattern = '(.*)'\n",
    "env_pattern = '.*-(.*)-v0'\n",
    "df = plot_value(experiment_dir, algo_pattern, env_pattern)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expert_cached_value(rl, env_name, pol_discount=0.99, eval_discount=1.00, seed=1234, episodes=100):\n",
    "    '''Rollout a cached expert policy for episodes.\n",
    "       WARNING: This will be slow or just break if policy is not in cache!'''\n",
    "    gen_policy, _sample, compute_value = config.RL_ALGORITHMS[rl]\n",
    "    policy, value = experiments._train_policy(rl, pol_discount, env_name, seed, None)\n",
    "    vmean, vse = value\n",
    "    print('Cached value: {:.3f} +/- {:.3f}'.format(vmean, 1.96 * vse))\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    rmean, rse = compute_value(env, policy, eval_discount, num_episodes=episodes, seed=seed)\n",
    "    print('Rollout value: {:.3f} +/- {:.3f}'.format(rmean, 1.96 * rse))\n",
    "    return (vmean, vse), (rmean, rse)\n",
    "\n",
    "def _policy_value(results_dir, rl, env_name, pol_discount, eval_discount, episodes, seed):\n",
    "    _gen_policy, _sample, compute_value = config.RL_ALGORITHMS[rl]\n",
    "    fname = osp.join(results_dir, 'policy.pkl')\n",
    "    print('Loading policy from ', fname)\n",
    "    policy = joblib.load(fname)\n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    mean, se = compute_value(env, policy, eval_discount, num_episodes=episodes, seed=seed)\n",
    "    print('Rollout value: {:.3f} +/- {:.3f}'.format(mean, 1.96 * se))\n",
    "    return mean, se\n",
    "\n",
    "def expert_value(experiment_dir, rl, env_name, pol_discount=0.99, eval_discount=1.00, episodes=100, seed=1234):\n",
    "    results_dir = osp.join(experiment_dir, 'expert', env_name, rl)\n",
    "    return _policy_value(results_dir, rl, env_name, pol_discount, eval_discount, episodes, seed)\n",
    "\n",
    "def irl_eval_value(experiment_dir, irl_name, num_traj, rl, env_name, pol_discount=0.99, eval_discount=1.00, episodes=100, seed=1234):\n",
    "    results_dir = osp.join(experiment_dir, 'eval', env_name, \n",
    "                           '{}:{}:{}'.format(irl_name, num_traj, num_traj), rl)\n",
    "    return _policy_value(results_dir, rl, env_name, pol_discount, eval_discount, episodes, seed)\n",
    "    \n",
    "def irl_value(experiment_dir, irl_name, env_name, num_traj, eval_discount=1.00, episodes=100):\n",
    "    _irl_algo, _reward_wrapper, compute_value = experiments.make_irl_algo(irl_name)\n",
    "    irl_dir = osp.join(experiment_dir, 'irl', irl_name)\n",
    "    if not os.path.exists(irl_dir):\n",
    "        raise FileNotFoundError(\"No result directory {}\".format(irl_dir))\n",
    "    \n",
    "    pop_fname = osp.join(irl_dir, str(num_traj), 'policies.pkl')\n",
    "    sin_fname = osp.join(irl_dir, env_name, str(num_traj), 'policy.pkl')\n",
    "    if os.path.exists(pop_fname):\n",
    "        policies = joblib.load(pop_fname)\n",
    "        print(policies.keys())\n",
    "        policy = policies[env_name]\n",
    "    elif os.path.exists(sin_fname):\n",
    "        policy = joblib.load(sin_fname)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Neither {} or {} exists\".format(pop_fname, sin_fname))\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    mean, se = compute_value(env, policy, discount=eval_discount, num_episodes=episodes)\n",
    "    print('Rollout value: {} +/- {}'.format(mean, 1.96 * se))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_cached_value('ppo_cts', 'Reacher-v2', episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_value(experiment_dir, 'ppo_cts', 'Reacher-v2', episodes=500, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irl_value(experiment_dir, 'airl', 'Reacher-v2', 1000, episodes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing rewards (gridworld only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_heatmaps(irl_algo, kind='inline', out_dir=None, shape=(9,9), **kwargs):\n",
    "    data = pd.read_pickle(osp.join(experiment_dir, 'results.pkl'))\n",
    "    rewards = data['rewards'][irl_algo]\n",
    "    if kind in ['inline', 'pdf']:\n",
    "        figs = myplots.gridworld_heatmap(rewards, shape)\n",
    "        if out_dir is None:\n",
    "            for fig in figs:\n",
    "                display(fig[1])\n",
    "        else:\n",
    "            myplots.save_figs(figs, out_dir)\n",
    "    elif kind == 'movie':\n",
    "        myplots.gridworld_heatmap_movie(out_dir, rewards, shape)\n",
    "    else:\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irl_algos = ['mce', 'mcec', 'mcep_reg1e0', 'mcep_reg1e-1', 'mcep_reg1e-2']\n",
    "for irl in irl_algos:\n",
    "    show_heatmaps(irl, kind='pdf', out_dir='figs/few-jungle/' + irl, shape=(9,9))\n",
    "    #show_heatmaps(irl, kind='movie', out_dir='figs/jungle/movies/' + irl)\n",
    "    #show_heatmaps(irl, kind='movie', out_dir='figs/jungle/' + irl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jungle experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jungle_types = collections.OrderedDict([\n",
    "    ('A', 'Soda'), \n",
    "    ('B', 'Water'), \n",
    "    ('A+B', 'Liquid')\n",
    "])\n",
    "jungle_envs = collections.OrderedDict([\n",
    "    (k, 'pirl/GridWorld-Jungle-9x9-{}-v0'.format(v))\n",
    "    for k, v in jungle_types.items()\n",
    "])\n",
    "default_algos = collections.OrderedDict([\n",
    "    ('mce', 'Single'),\n",
    "    ('mcec', 'Concat'),\n",
    "    ('mcep reg1e-1', 'Multi-task'),\n",
    "    ('value iteration', 'Oracle'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context([myplots.style('default'), myplots.style('twocol')]):\n",
    "    fig = myplots.gridworld_cartoon((9,9))\n",
    "    fig.savefig(osp.join(figs_dir, 'jungle', 'gt.pdf'))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reoptimized policy value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = plot_value(experiment_dir, algo_pattern, env_pattern)\n",
    "values.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_pattern = '(.*)'\n",
    "env_pattern = '.*-(.*)-v0'\n",
    "\n",
    "values = plot_value(experiment_dir, algo_pattern, env_pattern)\n",
    "values = values.rename(index={v: k for k, v in jungle_types.items()}, level=0)\n",
    "values = values.xs((1000, 'value_iteration'), level=('n', 'eval'))\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_comparison = values.rename(columns=default_algos).loc[:, default_algos.values()]\n",
    "with plt.style.context([myplots.style('default'), myplots.style('twocol')]):\n",
    "    fig = myplots.value_bar_chart_by_env(baseline_comparison, envs=jungle_types.keys(), relative='Oracle')\n",
    "    fig.savefig(osp.join(figs_dir, 'jungle', 'baseline_comparison.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_algorithms = collections.OrderedDict([\n",
    "    ('mcep reg0', '$\\lambda = 0$'),\n",
    "    ('mcep reg1e-2', '$\\lambda = 10^{-2}$'),\n",
    "    ('mcep reg1e-1', '$\\lambda = 10^{-1}$'),\n",
    "    ('mcep reg1e0', '$\\lambda = 10^0$'),\n",
    "    ('value iteration', 'Oracle'),\n",
    "])\n",
    "reg_comparison = values.rename(columns=reg_algorithms).loc[:, reg_algorithms.values()]\n",
    "with plt.style.context([myplots.style('default'), myplots.style('twocol')]):\n",
    "    fig = myplots.value_bar_chart_by_env(reg_comparison, envs=jungle_types.keys(), relative='Oracle')\n",
    "    fig.savefig(osp.join(figs_dir, 'jungle', 'reg_comparison.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_values = values.rename(columns=default_algos).loc[:, default_algos.values()]\n",
    "print(myplots.value_latex_table(table_values, envs=['A', 'B', 'A+B'], relative='Oracle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ad-hoc experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = osp.join(experiment_dir, 'results.pkl')\n",
    "data = pd.read_pickle(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'pirl/GridWorld-Jungle-9x9-Soda-v0'\n",
    "algo = 'mce'\n",
    "fig, axs = plt.subplots(1, 2, figsize=(13, 6))\n",
    "# zeroshot = data['rewards'][algo][1000][0][env_name]\n",
    "# oneshot = data['rewards'][algo][1000][1][env_name]\n",
    "# myplots._gridworld_heatmap(oneshot - zeroshot, (9,9), fmt='.2f', ax=axs[0])\n",
    "fiveshot = data['rewards'][algo][1000][5][env_name]\n",
    "hundredshot = data['rewards'][algo][1000][100][env_name]\n",
    "myplots._gridworld_heatmap(hundredshot - zeroshot, (9,9), fmt='.2f', ax=axs[1])\n",
    "\n",
    "env = gym.make(env_name)\n",
    "gt = env.unwrapped.reward\n",
    "env.close()\n",
    "r = data['rewards'][algo][1000]\n",
    "reward_delta = {k: np.linalg.norm(v[env_name] - np.mean(v[env_name]) - gt + np.mean(gt))\n",
    "                for k, v in r.items()}\n",
    "reward_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
